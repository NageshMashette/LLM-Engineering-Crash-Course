{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lexical Processing\n",
        "\n",
        "Lexical processing techniques are fundamental components of Natural Language Processing (NLP) that involve handling and processing textual data at the word or token level.\n",
        "\n",
        "##### Two key techniques\n",
        "\n",
        "1. **Tokenization**\n",
        "2. **Normalization**\n",
        "\n",
        "### 1. Tokenization\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/1*CdjbU3J5BYuIi-4WbWnKng.png)\n",
        "\n",
        "* **Tokenization** is the process of breaking down a text into smaller units, which are typically words or subwords.\n",
        "* The purpose of tokenization is to create a meaningful representation of the text that can be further analyzed or processed by NLP algorithms.\n",
        "* Tokens are the basic units of text used for various NLP tasks such as text classification, named entity recognition, and sentiment analysis."
      ],
      "metadata": {
        "id": "7iUwCFYp6TWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oozflnxo8rWU",
        "outputId": "e4c7dce3-f1fb-4b31-b87a-e95be7d9c0f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2zCtLTL6Odw",
        "outputId": "430afb7b-4511-4729-cad6-7bb02aa3933b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'smaller', 'units', ',', 'typically', 'words', 'or', 'subwords', ',', 'called', 'tokens', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Tokenization is the process of breaking down text into smaller units, typically words or subwords, called tokens.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Normalization\n",
        "\n",
        "**Normalization** in NLP involves transforming text into a standard, consistent format. Two common techniques for normalization are\n",
        "\n",
        "1. **Stemming**\n",
        "2. **Lemmatization**.\n",
        "\n",
        "Both techniques aim to reduce words to their base or root form, but they do so in different ways.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/0*6k_6zouDWBMWkehE)"
      ],
      "metadata": {
        "id": "OEeUP1ZN8nT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1 Stemming**\n",
        "\n",
        "* **Stemming** is the process of removing suffixes from words to reduce them to their root form. It operates by chopping off the end of words based on common patterns.\n",
        "\n",
        "![](https://qph.cf2.quoracdn.net/main-qimg-187b045c480fa7c0b16869daa0661b5a)\n",
        "\n",
        "Example using Porter Stemmer:"
      ],
      "metadata": {
        "id": "wvrqvZdi97Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"easily\", \"cats\", \"attractive\", \"programming\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Word ==> Stemmed Word\")\n",
        "print(\"=======================\")\n",
        "for word, stem_word in zip(words, stemmed_words):\n",
        "  print(f\"{word} ==> {stem_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abcNnI8k8jjp",
        "outputId": "cbdadf75-5789-4e56-8ef2-25d8afb8df20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word ==> Stemmed Word\n",
            "=======================\n",
            "running ==> run\n",
            "easily ==> easili\n",
            "cats ==> cat\n",
            "attractive ==> attract\n",
            "programming ==> program\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming reduces words to their base form, but it may not always result in a real word. For example, \"easily\" is stemmed to \"easili\", which is not a valid English word."
      ],
      "metadata": {
        "id": "GzlhuxOr-d5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Lemmatization:\n",
        "\n",
        "Lemmatization, on the other hand, reduces words to their base or dictionary form (lemma) while ensuring that the resulting lemma is a valid word.\n",
        "\n",
        "Example using WordNet Lemmatizer:"
      ],
      "metadata": {
        "id": "Vw99IRnD-hDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKK_crOi_OaA",
        "outputId": "8273f593-0c01-4c46-eede-4c5f2a111ad1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"easily\", \"cats\", \"attractive\", \"programming\"]\n",
        "\n",
        "# Define a function to map part-of-speech tags to WordNet tags\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "\n",
        "print(\"Word ==> Lemmitized Word\")\n",
        "print(\"=======================\")\n",
        "for word, lemmi_word in zip(words, lemmatized_words):\n",
        "  print(f\"{word} ==> {lemmi_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrTKV6LB-H3A",
        "outputId": "0b5d5bf8-12b6-4d79-8f4a-57ec1ce40b31"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word ==> Lemmitized Word\n",
            "=======================\n",
            "running ==> run\n",
            "easily ==> easily\n",
            "cats ==> cat\n",
            "attractive ==> attractive\n",
            "programming ==> program\n"
          ]
        }
      ]
    }
  ]
}